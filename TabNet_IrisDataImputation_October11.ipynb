{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6ec00e64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data (Before Missing Values):\n",
      "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
      "0                5.1               3.5                1.4               0.2\n",
      "1                4.9               3.0                1.4               0.2\n",
      "2                4.7               3.2                1.3               0.2\n",
      "3                4.6               3.1                1.5               0.2\n",
      "4                5.0               3.6                1.4               0.2\n",
      "\n",
      "Input Data with Missing Values:\n",
      "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
      "0                5.1               3.5                1.4               0.2\n",
      "1                4.9               3.0                1.4               0.2\n",
      "2                4.7               NaN                1.3               0.2\n",
      "3                NaN               3.1                1.5               0.2\n",
      "4                NaN               3.6                1.4               NaN\n",
      "\n",
      "After Normalizing Input Data:\n",
      "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
      "0           0.222222          0.625000           0.067797          0.041667\n",
      "1           0.166667          0.416667           0.067797          0.041667\n",
      "2           0.111111               NaN           0.050847          0.041667\n",
      "3                NaN          0.458333           0.084746          0.041667\n",
      "4                NaN          0.666667           0.067797               NaN\n",
      "\n",
      "Filled Missing Values with Zero Placeholder:\n",
      "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
      "0           0.222222          0.625000           0.067797          0.041667\n",
      "1           0.166667          0.416667           0.067797          0.041667\n",
      "2           0.111111          0.000000           0.050847          0.041667\n",
      "3           0.000000          0.458333           0.084746          0.041667\n",
      "4           0.000000          0.666667           0.067797          0.000000\n",
      "Device used : cpu\n",
      "No early stopping will be performed, last training weights will be used.\n",
      "epoch 0  | loss: 21.3723 |  0:00:00s\n",
      "epoch 1  | loss: 10.0537 |  0:00:00s\n",
      "epoch 2  | loss: 6.59571 |  0:00:00s\n",
      "epoch 3  | loss: 3.61779 |  0:00:00s\n",
      "epoch 4  | loss: 3.85039 |  0:00:00s\n",
      "epoch 5  | loss: 2.69323 |  0:00:00s\n",
      "epoch 6  | loss: 1.92737 |  0:00:00s\n",
      "epoch 7  | loss: 2.05505 |  0:00:00s\n",
      "epoch 8  | loss: 1.67672 |  0:00:00s\n",
      "epoch 9  | loss: 1.48792 |  0:00:00s\n",
      "epoch 10 | loss: 1.14399 |  0:00:00s\n",
      "epoch 11 | loss: 1.24825 |  0:00:00s\n",
      "epoch 12 | loss: 1.2211  |  0:00:00s\n",
      "epoch 13 | loss: 1.46699 |  0:00:00s\n",
      "epoch 14 | loss: 1.21505 |  0:00:00s\n",
      "epoch 15 | loss: 1.01832 |  0:00:00s\n",
      "epoch 16 | loss: 1.02612 |  0:00:00s\n",
      "epoch 17 | loss: 1.0873  |  0:00:00s\n",
      "epoch 18 | loss: 1.27887 |  0:00:00s\n",
      "epoch 19 | loss: 1.05705 |  0:00:00s\n",
      "epoch 20 | loss: 1.05933 |  0:00:00s\n",
      "epoch 21 | loss: 1.01849 |  0:00:00s\n",
      "epoch 22 | loss: 1.06264 |  0:00:00s\n",
      "epoch 23 | loss: 1.00123 |  0:00:00s\n",
      "epoch 24 | loss: 1.18317 |  0:00:00s\n",
      "epoch 25 | loss: 1.12181 |  0:00:00s\n",
      "epoch 26 | loss: 0.92586 |  0:00:00s\n",
      "epoch 27 | loss: 0.8527  |  0:00:00s\n",
      "epoch 28 | loss: 0.76763 |  0:00:00s\n",
      "epoch 29 | loss: 1.00776 |  0:00:00s\n",
      "epoch 30 | loss: 1.14548 |  0:00:00s\n",
      "epoch 31 | loss: 0.97866 |  0:00:00s\n",
      "epoch 32 | loss: 0.82914 |  0:00:00s\n",
      "epoch 33 | loss: 0.87126 |  0:00:00s\n",
      "epoch 34 | loss: 0.81886 |  0:00:00s\n",
      "epoch 35 | loss: 0.93825 |  0:00:00s\n",
      "epoch 36 | loss: 0.84413 |  0:00:00s\n",
      "epoch 37 | loss: 0.81641 |  0:00:00s\n",
      "epoch 38 | loss: 0.83829 |  0:00:00s\n",
      "epoch 39 | loss: 0.82097 |  0:00:00s\n",
      "\n",
      "Pretrained TabNet Model:\n",
      "\n",
      "True Missing Values:\n",
      "[[0.22222222 0.625      0.06779661 0.04166667]\n",
      " [0.16666667 0.41666667 0.06779661 0.04166667]\n",
      " [0.11111111 0.         0.05084746 0.04166667]\n",
      " [0.         0.45833333 0.08474576 0.04166667]\n",
      " [0.         0.66666667 0.06779661 0.        ]\n",
      " [0.         0.79166667 0.11864407 0.125     ]\n",
      " [0.08333333 0.58333333 0.         0.08333333]\n",
      " [0.19444444 0.58333333 0.08474576 0.04166667]\n",
      " [0.02777778 0.375      0.06779661 0.        ]\n",
      " [0.16666667 0.45833333 0.08474576 0.        ]\n",
      " [0.30555556 0.70833333 0.08474576 0.        ]\n",
      " [0.13888889 0.58333333 0.10169492 0.04166667]\n",
      " [0.13888889 0.         0.06779661 0.        ]\n",
      " [0.         0.41666667 0.01694915 0.        ]\n",
      " [0.41666667 0.83333333 0.03389831 0.04166667]\n",
      " [0.38888889 1.         0.08474576 0.125     ]\n",
      " [0.30555556 0.79166667 0.         0.125     ]\n",
      " [0.22222222 0.625      0.06779661 0.08333333]\n",
      " [0.         0.75       0.11864407 0.08333333]\n",
      " [0.22222222 0.75       0.08474576 0.08333333]\n",
      " [0.30555556 0.58333333 0.11864407 0.04166667]\n",
      " [0.22222222 0.         0.         0.125     ]\n",
      " [0.08333333 0.66666667 0.         0.04166667]\n",
      " [0.22222222 0.54166667 0.         0.16666667]\n",
      " [0.         0.         0.15254237 0.04166667]\n",
      " [0.19444444 0.41666667 0.10169492 0.04166667]\n",
      " [0.19444444 0.58333333 0.10169492 0.125     ]\n",
      " [0.         0.625      0.08474576 0.04166667]\n",
      " [0.25       0.58333333 0.06779661 0.04166667]\n",
      " [0.11111111 0.5        0.10169492 0.04166667]\n",
      " [0.13888889 0.         0.         0.04166667]\n",
      " [0.         0.58333333 0.08474576 0.125     ]\n",
      " [0.25       0.         0.         0.        ]\n",
      " [0.33333333 0.91666667 0.06779661 0.04166667]\n",
      " [0.16666667 0.         0.08474576 0.04166667]\n",
      " [0.19444444 0.5        0.03389831 0.04166667]\n",
      " [0.33333333 0.625      0.         0.04166667]\n",
      " [0.16666667 0.66666667 0.06779661 0.        ]\n",
      " [0.02777778 0.41666667 0.05084746 0.        ]\n",
      " [0.         0.58333333 0.08474576 0.        ]\n",
      " [0.19444444 0.625      0.05084746 0.08333333]\n",
      " [0.05555556 0.125      0.05084746 0.08333333]\n",
      " [0.02777778 0.5        0.05084746 0.04166667]\n",
      " [0.         0.625      0.10169492 0.20833333]\n",
      " [0.         0.75       0.15254237 0.125     ]\n",
      " [0.13888889 0.         0.06779661 0.08333333]\n",
      " [0.22222222 0.         0.10169492 0.04166667]\n",
      " [0.08333333 0.5        0.06779661 0.04166667]\n",
      " [0.         0.70833333 0.08474576 0.04166667]\n",
      " [0.19444444 0.54166667 0.06779661 0.04166667]\n",
      " [0.         0.5        0.62711864 0.54166667]\n",
      " [0.58333333 0.5        0.59322034 0.        ]\n",
      " [0.72222222 0.45833333 0.         0.58333333]\n",
      " [0.         0.125      0.50847458 0.5       ]\n",
      " [0.61111111 0.         0.61016949 0.58333333]\n",
      " [0.38888889 0.33333333 0.59322034 0.        ]\n",
      " [0.         0.54166667 0.62711864 0.625     ]\n",
      " [0.16666667 0.16666667 0.38983051 0.        ]\n",
      " [0.63888889 0.375      0.61016949 0.5       ]\n",
      " [0.         0.29166667 0.49152542 0.54166667]\n",
      " [0.19444444 0.         0.42372881 0.375     ]\n",
      " [0.44444444 0.         0.54237288 0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.5        0.375      0.62711864 0.        ]\n",
      " [0.         0.375      0.         0.5       ]\n",
      " [0.66666667 0.45833333 0.         0.54166667]\n",
      " [0.36111111 0.         0.59322034 0.58333333]\n",
      " [0.41666667 0.29166667 0.52542373 0.375     ]\n",
      " [0.52777778 0.         0.59322034 0.58333333]\n",
      " [0.36111111 0.20833333 0.49152542 0.        ]\n",
      " [0.44444444 0.5        0.6440678  0.70833333]\n",
      " [0.5        0.33333333 0.50847458 0.5       ]\n",
      " [0.55555556 0.20833333 0.66101695 0.58333333]\n",
      " [0.5        0.33333333 0.62711864 0.45833333]\n",
      " [0.58333333 0.         0.         0.        ]\n",
      " [0.63888889 0.         0.57627119 0.54166667]\n",
      " [0.69444444 0.33333333 0.6440678  0.54166667]\n",
      " [0.66666667 0.41666667 0.6779661  0.66666667]\n",
      " [0.47222222 0.         0.59322034 0.58333333]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.33333333 0.16666667 0.47457627 0.41666667]\n",
      " [0.33333333 0.16666667 0.45762712 0.375     ]\n",
      " [0.41666667 0.29166667 0.49152542 0.45833333]\n",
      " [0.47222222 0.29166667 0.69491525 0.625     ]\n",
      " [0.30555556 0.41666667 0.59322034 0.58333333]\n",
      " [0.         0.58333333 0.59322034 0.625     ]\n",
      " [0.66666667 0.45833333 0.         0.58333333]\n",
      " [0.55555556 0.125      0.57627119 0.5       ]\n",
      " [0.36111111 0.41666667 0.52542373 0.5       ]\n",
      " [0.33333333 0.20833333 0.50847458 0.5       ]\n",
      " [0.33333333 0.25       0.57627119 0.45833333]\n",
      " [0.5        0.41666667 0.61016949 0.54166667]\n",
      " [0.41666667 0.25       0.50847458 0.45833333]\n",
      " [0.19444444 0.125      0.38983051 0.        ]\n",
      " [0.         0.29166667 0.54237288 0.5       ]\n",
      " [0.         0.41666667 0.54237288 0.45833333]\n",
      " [0.38888889 0.375      0.         0.5       ]\n",
      " [0.52777778 0.375      0.55932203 0.5       ]\n",
      " [0.22222222 0.20833333 0.33898305 0.41666667]\n",
      " [0.38888889 0.33333333 0.52542373 0.5       ]\n",
      " [0.         0.54166667 0.84745763 1.        ]\n",
      " [0.41666667 0.29166667 0.69491525 0.75      ]\n",
      " [0.         0.41666667 0.83050847 0.83333333]\n",
      " [0.55555556 0.         0.77966102 0.        ]\n",
      " [0.61111111 0.41666667 0.         0.875     ]\n",
      " [0.91666667 0.41666667 0.94915254 0.83333333]\n",
      " [0.16666667 0.         0.         0.66666667]\n",
      " [0.83333333 0.375      0.89830508 0.        ]\n",
      " [0.66666667 0.20833333 0.81355932 0.70833333]\n",
      " [0.80555556 0.66666667 0.86440678 1.        ]\n",
      " [0.61111111 0.         0.69491525 0.79166667]\n",
      " [0.         0.29166667 0.72881356 0.75      ]\n",
      " [0.69444444 0.41666667 0.76271186 0.83333333]\n",
      " [0.38888889 0.20833333 0.6779661  0.79166667]\n",
      " [0.41666667 0.         0.69491525 0.95833333]\n",
      " [0.58333333 0.5        0.         0.91666667]\n",
      " [0.61111111 0.41666667 0.         0.70833333]\n",
      " [0.94444444 0.75       0.96610169 0.        ]\n",
      " [0.94444444 0.25       1.         0.91666667]\n",
      " [0.47222222 0.08333333 0.6779661  0.58333333]\n",
      " [0.72222222 0.         0.         0.91666667]\n",
      " [0.         0.33333333 0.66101695 0.79166667]\n",
      " [0.94444444 0.         0.96610169 0.79166667]\n",
      " [0.         0.29166667 0.66101695 0.70833333]\n",
      " [0.66666667 0.54166667 0.         0.83333333]\n",
      " [0.80555556 0.5        0.84745763 0.70833333]\n",
      " [0.52777778 0.33333333 0.         0.70833333]\n",
      " [0.5        0.41666667 0.         0.        ]\n",
      " [0.58333333 0.         0.77966102 0.83333333]\n",
      " [0.80555556 0.41666667 0.81355932 0.625     ]\n",
      " [0.86111111 0.         0.         0.        ]\n",
      " [1.         0.75       0.91525424 0.79166667]\n",
      " [0.58333333 0.         0.77966102 0.        ]\n",
      " [0.55555556 0.33333333 0.         0.        ]\n",
      " [0.5        0.25       0.77966102 0.        ]\n",
      " [0.94444444 0.41666667 0.         0.91666667]\n",
      " [0.         0.58333333 0.         0.95833333]\n",
      " [0.         0.45833333 0.76271186 0.        ]\n",
      " [0.         0.41666667 0.6440678  0.70833333]\n",
      " [0.72222222 0.45833333 0.74576271 0.83333333]\n",
      " [0.66666667 0.45833333 0.77966102 0.        ]\n",
      " [0.72222222 0.         0.69491525 0.        ]\n",
      " [0.41666667 0.29166667 0.69491525 0.75      ]\n",
      " [0.69444444 0.         0.83050847 0.        ]\n",
      " [0.66666667 0.54166667 0.79661017 1.        ]\n",
      " [0.66666667 0.41666667 0.         0.91666667]\n",
      " [0.55555556 0.         0.6779661  0.75      ]\n",
      " [0.         0.41666667 0.71186441 0.79166667]\n",
      " [0.         0.58333333 0.         0.91666667]\n",
      " [0.         0.41666667 0.69491525 0.        ]]\n",
      "\n",
      "Reconstructed Data:\n",
      "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
      "0           0.222222          0.625000           0.067797          0.041667\n",
      "1           0.166667          0.416667           0.067797          0.041667\n",
      "2           0.111111          0.431033           0.050847          0.041667\n",
      "3           0.344455          0.458333           0.084746          0.041667\n",
      "4           0.339012          0.666667           0.067797          0.033917\n",
      "\n",
      "Imputed Values:\n",
      "[[0.22222222 0.625      0.06779661 0.04166667]\n",
      " [0.16666667 0.41666667 0.06779661 0.04166667]\n",
      " [0.11111111 0.43103307 0.05084746 0.04166667]\n",
      " [0.34445548 0.45833333 0.08474576 0.04166667]\n",
      " [0.33901182 0.66666667 0.06779661 0.03391723]\n",
      " [0.34680477 0.79166667 0.11864407 0.125     ]\n",
      " [0.08333333 0.58333333 0.32782656 0.08333333]\n",
      " [0.19444444 0.58333333 0.08474576 0.04166667]\n",
      " [0.02777778 0.375      0.06779661 0.03444698]\n",
      " [0.16666667 0.45833333 0.08474576 0.03537096]\n",
      " [0.30555556 0.70833333 0.08474576 0.03630255]\n",
      " [0.13888889 0.58333333 0.10169492 0.04166667]\n",
      " [0.13888889 0.43299198 0.06779661 0.0376257 ]\n",
      " [0.3430143  0.41666667 0.01694915 0.03401072]\n",
      " [0.41666667 0.83333333 0.03389831 0.04166667]\n",
      " [0.38888889 1.         0.08474576 0.125     ]\n",
      " [0.30555556 0.79166667 0.35379332 0.125     ]\n",
      " [0.22222222 0.625      0.06779661 0.08333333]\n",
      " [0.34413481 0.75       0.11864407 0.08333333]\n",
      " [0.22222222 0.75       0.08474576 0.08333333]\n",
      " [0.30555556 0.58333333 0.11864407 0.04166667]\n",
      " [0.22222222 0.42395371 0.29807121 0.125     ]\n",
      " [0.08333333 0.66666667 0.33113295 0.04166667]\n",
      " [0.22222222 0.54166667 0.33451205 0.16666667]\n",
      " [0.34703004 0.43665385 0.15254237 0.04166667]\n",
      " [0.19444444 0.41666667 0.10169492 0.04166667]\n",
      " [0.19444444 0.58333333 0.10169492 0.125     ]\n",
      " [0.34265405 0.625      0.08474576 0.04166667]\n",
      " [0.25       0.58333333 0.06779661 0.04166667]\n",
      " [0.11111111 0.5        0.10169492 0.04166667]\n",
      " [0.13888889 0.42855936 0.29582515 0.04166667]\n",
      " [0.3500914  0.58333333 0.08474576 0.125     ]\n",
      " [0.25       0.42860878 0.29802302 0.03916098]\n",
      " [0.33333333 0.91666667 0.06779661 0.04166667]\n",
      " [0.16666667 0.43160075 0.08474576 0.04166667]\n",
      " [0.19444444 0.5        0.03389831 0.04166667]\n",
      " [0.33333333 0.625      0.33557272 0.04166667]\n",
      " [0.16666667 0.66666667 0.06779661 0.03517824]\n",
      " [0.02777778 0.41666667 0.05084746 0.03428657]\n",
      " [0.33949894 0.58333333 0.08474576 0.03396478]\n",
      " [0.19444444 0.625      0.05084746 0.08333333]\n",
      " [0.05555556 0.125      0.05084746 0.08333333]\n",
      " [0.02777778 0.5        0.05084746 0.04166667]\n",
      " [0.35571754 0.625      0.10169492 0.20833333]\n",
      " [0.34684953 0.75       0.15254237 0.125     ]\n",
      " [0.13888889 0.42967144 0.06779661 0.08333333]\n",
      " [0.22222222 0.43141139 0.10169492 0.04166667]\n",
      " [0.08333333 0.5        0.06779661 0.04166667]\n",
      " [0.34176722 0.70833333 0.08474576 0.04166667]\n",
      " [0.19444444 0.54166667 0.06779661 0.04166667]\n",
      " [0.36768216 0.5        0.62711864 0.54166667]\n",
      " [0.58333333 0.5        0.59322034 0.04441917]\n",
      " [0.72222222 0.45833333 0.36713603 0.58333333]\n",
      " [0.37163997 0.125      0.50847458 0.5       ]\n",
      " [0.61111111 0.4233571  0.61016949 0.58333333]\n",
      " [0.38888889 0.33333333 0.59322034 0.04341699]\n",
      " [0.37409922 0.54166667 0.62711864 0.625     ]\n",
      " [0.16666667 0.16666667 0.38983051 0.03864108]\n",
      " [0.63888889 0.375      0.61016949 0.5       ]\n",
      " [0.37480637 0.29166667 0.49152542 0.54166667]\n",
      " [0.19444444 0.43244427 0.42372881 0.375     ]\n",
      " [0.44444444 0.44335228 0.54237288 0.0438375 ]\n",
      " [0.35036904 0.43205923 0.29238158 0.03674892]\n",
      " [0.5        0.375      0.62711864 0.04475901]\n",
      " [0.38603702 0.375      0.33413246 0.5       ]\n",
      " [0.66666667 0.45833333 0.36336738 0.54166667]\n",
      " [0.36111111 0.42852151 0.59322034 0.58333333]\n",
      " [0.41666667 0.29166667 0.52542373 0.375     ]\n",
      " [0.52777778 0.42476079 0.59322034 0.58333333]\n",
      " [0.36111111 0.20833333 0.49152542 0.04160455]\n",
      " [0.44444444 0.5        0.6440678  0.70833333]\n",
      " [0.5        0.33333333 0.50847458 0.5       ]\n",
      " [0.55555556 0.20833333 0.66101695 0.58333333]\n",
      " [0.5        0.33333333 0.62711864 0.45833333]\n",
      " [0.58333333 0.42246836 0.30400085 0.04267307]\n",
      " [0.63888889 0.42285582 0.57627119 0.54166667]\n",
      " [0.69444444 0.33333333 0.6440678  0.54166667]\n",
      " [0.66666667 0.41666667 0.6779661  0.66666667]\n",
      " [0.47222222 0.42606032 0.59322034 0.58333333]\n",
      " [0.35036904 0.43205923 0.29238158 0.03674892]\n",
      " [0.33333333 0.16666667 0.47457627 0.41666667]\n",
      " [0.33333333 0.16666667 0.45762712 0.375     ]\n",
      " [0.41666667 0.29166667 0.49152542 0.45833333]\n",
      " [0.47222222 0.29166667 0.69491525 0.625     ]\n",
      " [0.30555556 0.41666667 0.59322034 0.58333333]\n",
      " [0.37511769 0.58333333 0.59322034 0.625     ]\n",
      " [0.66666667 0.45833333 0.36584681 0.58333333]\n",
      " [0.55555556 0.125      0.57627119 0.5       ]\n",
      " [0.36111111 0.41666667 0.52542373 0.5       ]\n",
      " [0.33333333 0.20833333 0.50847458 0.5       ]\n",
      " [0.33333333 0.25       0.57627119 0.45833333]\n",
      " [0.5        0.41666667 0.61016949 0.54166667]\n",
      " [0.41666667 0.25       0.50847458 0.45833333]\n",
      " [0.19444444 0.125      0.38983051 0.03899671]\n",
      " [0.36877224 0.29166667 0.54237288 0.5       ]\n",
      " [0.36467957 0.41666667 0.54237288 0.45833333]\n",
      " [0.38888889 0.375      0.34545135 0.5       ]\n",
      " [0.52777778 0.375      0.55932203 0.5       ]\n",
      " [0.22222222 0.20833333 0.33898305 0.41666667]\n",
      " [0.38888889 0.33333333 0.52542373 0.5       ]\n",
      " [0.39169183 0.54166667 0.84745763 1.        ]\n",
      " [0.41666667 0.29166667 0.69491525 0.75      ]\n",
      " [0.3802028  0.41666667 0.83050847 0.83333333]\n",
      " [0.55555556 0.44278604 0.77966102 0.05100355]\n",
      " [0.61111111 0.41666667 0.37581977 0.875     ]\n",
      " [0.91666667 0.41666667 0.94915254 0.83333333]\n",
      " [0.16666667 0.40131024 0.30618548 0.66666667]\n",
      " [0.83333333 0.375      0.89830508 0.0536757 ]\n",
      " [0.66666667 0.20833333 0.81355932 0.70833333]\n",
      " [0.80555556 0.66666667 0.86440678 1.        ]\n",
      " [0.61111111 0.41939008 0.69491525 0.79166667]\n",
      " [0.38055596 0.29166667 0.72881356 0.75      ]\n",
      " [0.69444444 0.41666667 0.76271186 0.83333333]\n",
      " [0.38888889 0.20833333 0.6779661  0.79166667]\n",
      " [0.41666667 0.41833821 0.69491525 0.95833333]\n",
      " [0.58333333 0.5        0.38768107 0.91666667]\n",
      " [0.61111111 0.41666667 0.36707199 0.70833333]\n",
      " [0.94444444 0.75       0.96610169 0.04829776]\n",
      " [0.94444444 0.25       1.         0.91666667]\n",
      " [0.47222222 0.08333333 0.6779661  0.58333333]\n",
      " [0.72222222 0.35297608 0.31568518 0.91666667]\n",
      " [0.38716006 0.33333333 0.66101695 0.79166667]\n",
      " [0.94444444 0.41927069 0.96610169 0.79166667]\n",
      " [0.38065797 0.29166667 0.66101695 0.70833333]\n",
      " [0.66666667 0.54166667 0.38992357 0.83333333]\n",
      " [0.80555556 0.5        0.84745763 0.70833333]\n",
      " [0.52777778 0.33333333 0.3555457  0.70833333]\n",
      " [0.5        0.41666667 0.32520199 0.03773727]\n",
      " [0.58333333 0.42195863 0.77966102 0.83333333]\n",
      " [0.80555556 0.41666667 0.81355932 0.625     ]\n",
      " [0.86111111 0.41606945 0.307437   0.04610787]\n",
      " [1.         0.75       0.91525424 0.79166667]\n",
      " [0.58333333 0.44230741 0.77966102 0.05107348]\n",
      " [0.55555556 0.33333333 0.32161081 0.03866228]\n",
      " [0.5        0.25       0.77966102 0.04947229]\n",
      " [0.94444444 0.41666667 0.38257349 0.91666667]\n",
      " [0.40787515 0.58333333 0.38274783 0.95833333]\n",
      " [0.3017711  0.45833333 0.76271186 0.04538988]\n",
      " [0.38061246 0.41666667 0.6440678  0.70833333]\n",
      " [0.72222222 0.45833333 0.74576271 0.83333333]\n",
      " [0.66666667 0.45833333 0.77966102 0.0492347 ]\n",
      " [0.72222222 0.4402214  0.69491525 0.04883629]\n",
      " [0.41666667 0.29166667 0.69491525 0.75      ]\n",
      " [0.69444444 0.43976182 0.83050847 0.05313733]\n",
      " [0.66666667 0.54166667 0.79661017 1.        ]\n",
      " [0.66666667 0.41666667 0.37866849 0.91666667]\n",
      " [0.55555556 0.42156953 0.6779661  0.75      ]\n",
      " [0.38407138 0.41666667 0.71186441 0.79166667]\n",
      " [0.40548924 0.58333333 0.38035354 0.91666667]\n",
      " [0.30760634 0.41666667 0.69491525 0.04339452]]\n",
      "\n",
      "Original Data (Before Missing Values):\n",
      "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
      "0                5.1               3.5                1.4               0.2\n",
      "1                4.9               3.0                1.4               0.2\n",
      "2                4.7               3.2                1.3               0.2\n",
      "3                4.6               3.1                1.5               0.2\n",
      "4                5.0               3.6                1.4               0.2\n",
      "\n",
      "Denormalized Reconstructed Data:\n",
      "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
      "0           5.100000          3.500000                1.4          0.200000\n",
      "1           4.900000          3.000000                1.4          0.200000\n",
      "2           4.700000          3.034479                1.3          0.200000\n",
      "3           5.540040          3.100000                1.5          0.200000\n",
      "4           5.520443          3.600000                1.4          0.181401\n",
      "\n",
      "RMSE between Original Data and Denormalized Reconstructed Data: 0.5451450277007475\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "data = iris.data\n",
    "columns = iris.feature_names\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "# Make a copy of the original data before introducing missing values\n",
    "original_data = df.copy()\n",
    "print(\"Original Data (Before Missing Values):\")\n",
    "print(original_data.head())\n",
    "\n",
    "# Introduce artificial missing values\n",
    "missing_fraction = 0.2\n",
    "mask = np.random.rand(*data.shape) < missing_fraction\n",
    "df[mask] = np.nan\n",
    "\n",
    "# Print the input dataset with missing values\n",
    "print(\"\\nInput Data with Missing Values:\")\n",
    "print(df.head())\n",
    "\n",
    "# Normalize the dataset\n",
    "scaler = MinMaxScaler()\n",
    "numeric_cols = df.columns\n",
    "df[numeric_cols] = scaler.fit_transform(df[numeric_cols])\n",
    "\n",
    "print(\"\\nAfter Normalizing Input Data:\")\n",
    "print(df.head())\n",
    "\n",
    "# Fill missing values with zero\n",
    "df[numeric_cols] = df[numeric_cols].fillna(0)\n",
    "\n",
    "print(\"\\nFilled Missing Values with Zero Placeholder:\")\n",
    "print(df.head())\n",
    "\n",
    "# Define and pretrain the TabNet model\n",
    "pretrained_model = TabNetPretrainer(\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=2e-2),\n",
    "    mask_type='sparsemax',\n",
    "    #n_d=32,  # Adjust the number of decision steps as needed\n",
    "    #n_a=32   # Adjust the number of features shared as needed\n",
    ")\n",
    "\n",
    "max_epochs = 40\n",
    "pretrained_model.fit(\n",
    "    df[numeric_cols].values,\n",
    "    max_epochs=max_epochs\n",
    ")\n",
    "\n",
    "print(\"\\nPretrained TabNet Model:\")\n",
    "\n",
    "def tabnet_recon(df, network, missing_value_placeholder=0, numeric_cols=None, scaler=None):\n",
    "    # Ensure numeric_cols and scaler are provided\n",
    "    if numeric_cols is None or scaler is None:\n",
    "        raise ValueError(\"Please provide numeric_cols and scaler.\")\n",
    "    \n",
    "    # Copy the input data\n",
    "    df_tab = df.copy()\n",
    "    \n",
    "    # Identify the rows and columns where the placeholder value exists\n",
    "    missing_mask = (df_tab[numeric_cols] == missing_value_placeholder)\n",
    "    \n",
    "    # Normalize the input data\n",
    "    df_tab[numeric_cols] = scaler.transform(df_tab[numeric_cols])\n",
    "    \n",
    "    # Convert input data to tensors for use in the TabNet network\n",
    "    inputData = torch.tensor(df_tab[numeric_cols].values, dtype=torch.float32)\n",
    "    \n",
    "    # Pass the input data through the TabNet network\n",
    "    results = network.predict(inputData)\n",
    "    \n",
    "    # Ensure results is a NumPy array\n",
    "    if isinstance(results, tuple):\n",
    "        results = results[0]  # Use the first element of the tuple\n",
    "    elif isinstance(results, torch.Tensor):\n",
    "        results = results.detach().numpy()  # Convert to NumPy array\n",
    "        \n",
    "    df_tab[numeric_cols] = results\n",
    "    \n",
    "    # Apply the reconstructed values only to the missing positions\n",
    "    df_tab[numeric_cols] = np.where(missing_mask, df_tab[numeric_cols], df[numeric_cols] )\n",
    "    \n",
    "    return df_tab\n",
    "\n",
    "# Extract true missing values before filling\n",
    "true_missing_values = df[numeric_cols].values\n",
    "\n",
    "print('\\nTrue Missing Values:')\n",
    "print(true_missing_values)\n",
    "\n",
    "\n",
    "# Reconstruct missing values using the pretrained model\n",
    "reconstructed_data = tabnet_recon(\n",
    "    df,\n",
    "    network=pretrained_model,\n",
    "    numeric_cols=numeric_cols,  # Provide the numeric_cols\n",
    "    scaler=scaler  # Provide the scaler\n",
    ")\n",
    "\n",
    "print('\\nReconstructed Data:')\n",
    "print(reconstructed_data.head())\n",
    "\n",
    "# Extract imputed values\n",
    "imputed_values = reconstructed_data[numeric_cols].values\n",
    "\n",
    "print('\\nImputed Values:')\n",
    "print(imputed_values)\n",
    "\n",
    "\n",
    "# Denormalize the reconstructed data\n",
    "reconstructed_data[numeric_cols] = scaler.inverse_transform(reconstructed_data[numeric_cols])\n",
    "\n",
    "# Print the original data (before missing values) and reconstructed data\n",
    "print(\"\\nOriginal Data (Before Missing Values):\")\n",
    "print(original_data.head())\n",
    "\n",
    "print(\"\\nDenormalized Reconstructed Data:\")\n",
    "print(reconstructed_data.head())\n",
    "\n",
    "# Calculate RMSE\n",
    "rmse = np.sqrt(np.mean((original_data[numeric_cols].values - reconstructed_data[numeric_cols].values) ** 2))\n",
    "\n",
    "# Print the RMSE\n",
    "print(\"\\nRMSE between Original Data and Denormalized Reconstructed Data:\", rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d5efd3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefbf29d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
